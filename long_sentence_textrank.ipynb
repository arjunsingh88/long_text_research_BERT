{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: new model + Extractive summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import time \n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "pd.options.display.float_format = '{:.10f}'.format\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import pytextrank\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "from math import sqrt\n",
    "from operator import itemgetter\n",
    "import spacy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load newly trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"nlptown/bert-base-multilingual-uncased-sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize our model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast = True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Custom functions to increase text readability + generate extractive summary + compute sentiment "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cleaning steps include:\n",
    "    # Removing non-alphanumeric characters and specific French characters.\n",
    "    # Removing URLs.\n",
    "    # Removing \"rt\" (retweet) tags.\n",
    "    # Removing excessive spaces and replacing consecutive spaces with a single space.\n",
    "    # Stripping leading and trailing whitespace.\n",
    "    # Replacing newline characters with spaces.\n",
    "    # Removing the possessive form \"'s\" by replacing it with \"s\".\n",
    "    # If any error occurs during the cleaning process, an error message is printed, and the original text is returned without any modifications.\n",
    "\n",
    "def clean_string(text:str) -> str:\n",
    "    try: \n",
    "        # utf8_apostrophe = b'\\xe2\\x80\\x99'.decode(\"utf8\")\n",
    "        DATE_REGEX = r\"\\d+\\/\\d+\\/\\d+\" # Regular expression pattern for matching date format\n",
    "        text = re.sub(r\"([^0-9A-Za-z àâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ\\~\\°\\&\\“\\%\\'\\:\\;\\!\\-\\’\\\"\\.\\,\\?\\€\\$\\t\\n`(\\d+\\/\\d+\\/\\d+)``\\(.*\\)`])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "        # Remove unwanted characters, URLs, and RT tags from the text\n",
    "        text = re.sub(' {2,}', ' ', text) # Remove inconsistent spaces (2 or more spaces become a single space)\n",
    "        text = re.sub(' \\. ', '.', text) # Remove inconsistent spaces around periods\n",
    "        # text = re.sub(utf8_apostrophe, \"'\", text)  # Substitute UTF-8 apostrophe with a regular apostrophe\n",
    "        text = text.strip()  # Strip leading and trailing whitespace\n",
    "        text = re.sub(r\"\\n\", \" \", text) # Replace newline characters with a space\n",
    "        text = re.sub(r\"\\'s\\b\", \"s\", text) # Replace \"'s\" with \"s\" (possessive form)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning text: {e}\")\n",
    "        return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithms\n",
    "##### Algorithm 1: Text ranking Algorithm - top ranking sentences <br /> Algorithm 2: Biased/entity driven Text ranking Algorithm - top ranking sentences\n",
    "\n",
    "`Note: Only run one algorithm at a time, variables are same for ease of running`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the algorithm you want to run\n",
    "# algo_name = 'textrank' \n",
    "algo_name = 'biasedtextrank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load(\"en_core_web_trf\")\n",
    "nlp_fr = spacy.load(\"fr_dep_news_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biasedtextrank algorithm initnalized\n"
     ]
    }
   ],
   "source": [
    "if algo_name in ['textrank','biasedtextrank']:\n",
    "    print(f'{algo_name} algorithm initnalized')\n",
    "    nlp_en.add_pipe(algo_name)\n",
    "    nlp_fr.add_pipe(algo_name)\n",
    "else:\n",
    "    raise Exception(\"Wrong algorithm inintalized\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Extractive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extractive_summary(text:str, focus:str, key_phrases:int, no_of_sentences:int, lang:str, algo:str) -> str:\n",
    "    \"\"\"\n",
    "    Extractive text summarization using the specified algorithm. \n",
    "    The algorithm in consideration is either Textrank and Biased-Textrank\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text (for our case the complete html content) to summarize.\n",
    "        focus (str): The focus keyword (for our case the Organizational entity) for biased summarization.\n",
    "        key_phrases (int): The number of key phrases (topics) to consider. \n",
    "        no_of_sentences (int): The number of sentences to include in the summary. For the purpose of this work it is set to 12\n",
    "        lang (str): The language of the input text ('fr' for French, 'en' for English).\n",
    "        algo (str): The algorithm to use for summarization ('textrank' or 'biasedtextrank').\n",
    "                    1. TextRank is an unsupervised graph-based ranking algorithm used for text summarization.\n",
    "                    2. Biased TextRank is an extension of the TextRank algorithm that allows biased summarization based on a specific keyword.\n",
    "\n",
    "    Returns:\n",
    "        str: The summarized text.\n",
    "\n",
    "    \"\"\"\n",
    "    limit_phrases = key_phrases\n",
    "    limit_sentences = no_of_sentences\n",
    "    \n",
    "    # Choose the appropriate language model based on the input language\n",
    "    nlp = nlp_fr if lang=='fr' else nlp_en\n",
    "    doc = nlp(text)\n",
    "\n",
    "    if algo == 'textrank':\n",
    "        # Use TextRank algorithm for summarization\n",
    "        # print(\"Algorithm: \", algo)\n",
    "        pass\n",
    "    elif algo == 'biasedtextrank':\n",
    "        # Use Biased TextRank algorithm for summarization\n",
    "        # Set the focus keyword and bias to influence the summary\n",
    "        # The bias range selects sentences that are more focused on the organizational entity\n",
    "        # print(\"Algorithm: \", algo)\n",
    "        doc._.textrank.change_focus(focus, bias=10.0,  default_bias=0.0)\n",
    "\n",
    "    # Initialize sentence boundaries and associated phrase IDs\n",
    "    sent_bounds = [ [s.start, s.end, set([])] for s in doc.sents ]\n",
    "\n",
    "    # unit_vector, sent_vector = _get_vectors(doc,no_of_key_phrases=5)\n",
    "    \n",
    "    phrase_id = 0\n",
    "    unit_vector = []\n",
    "\n",
    "    for p in doc._.phrases:\n",
    "        # print(f'{phrase_id}, {p.text}, {p.rank}')\n",
    "        # Collect the rank of each phrase to build the unit vector\n",
    "        unit_vector.append(p.rank)\n",
    "\n",
    "        for chunk in p.chunks:\n",
    "            # print(f'{chunk.start}, {chunk.end}')\n",
    "            # Associate each phrase with the corresponding sentence boundaries\n",
    "\n",
    "            for sent_start, sent_end, sent_vector in sent_bounds:\n",
    "                if chunk.start >= sent_start and chunk.end <= sent_end:\n",
    "                    # print({sent_start}, {chunk.start}, {chunk.end}, {sent_end})\n",
    "                    sent_vector.add(phrase_id)\n",
    "                    break\n",
    "\n",
    "        phrase_id += 1\n",
    "\n",
    "        if phrase_id == limit_phrases:\n",
    "            break\n",
    "\n",
    "    sum_ranks = sum(unit_vector)\n",
    "    unit_vector = [ rank/sum_ranks for rank in unit_vector ]\n",
    "\n",
    "    sent_rank = {}\n",
    "    sent_id = 0\n",
    "\n",
    "    for sent_start, sent_end, sent_vector in sent_bounds:\n",
    "        # print(sent_vector)\n",
    "        # Calculate the rank of each sentence based on the unit vector\n",
    "        sum_sq = 0.0\n",
    "        \n",
    "        for phrase_id in range(len(unit_vector)):\n",
    "            # print(phrase_id, unit_vector[phrase_id])\n",
    "            \n",
    "            if phrase_id not in sent_vector:\n",
    "                sum_sq += unit_vector[phrase_id]**2.0\n",
    "\n",
    "        sent_rank[sent_id] = sqrt(sum_sq)\n",
    "        sent_id += 1\n",
    "\n",
    "    \n",
    "\n",
    "    sent_text = {}\n",
    "    sent_id = 0\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        # Store the text of each sentence for later retrieval\n",
    "        sent_text[sent_id] = sent.text\n",
    "        sent_id += 1\n",
    "\n",
    "    num_sent = 0\n",
    "\n",
    "    top_10_sent_id = []\n",
    "    for sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n",
    "        # Sort the sentences based on their ranks and select the top ones\n",
    "        # print(sent_id, sent_text[sent_id])\n",
    "        top_10_sent_id.append(sent_id)\n",
    "        num_sent += 1\n",
    "        \n",
    "        if num_sent == limit_sentences:\n",
    "            break\n",
    "\n",
    "    top_10_sent_id.sort()\n",
    "    # print('\\n'.join([f\"{sent_id}: {sent_text[sent_id]}\" for sent_id in sent_text if sent_id in top_10_sent_id]))\n",
    "\n",
    "    return ' '.join([sent_text[sent_id] for sent_id in sent_text if sent_id in top_10_sent_id])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: function to compute sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_sentiment(html_content:str, title_sentences:str, name:str, model, tokenizer ):\n",
    "\n",
    "    # A quick cleaninig of html_content and title\n",
    "    html_content = clean_string(text = html_content)  #\n",
    "    title_sentences = clean_string(text = title_sentences) #\n",
    "    html_content = re.sub(re.escape(title_sentences), \"\", html_content) # this code eliminates the duplicated title text in html_content\n",
    "    content_sentences = nltk.sent_tokenize(html_content)\n",
    "    content_inputs = tokenizer(content_sentences,  padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    #  torch.no_grad() = context manager that temporarily disables gradient calculation, which reduces memory usage and \n",
    "    #  speeds up computation when you only need to perform forward passes through the network (e.g., during inference).\n",
    "    with torch.no_grad(): \n",
    "        content_outputs = model(**content_inputs)\n",
    "\n",
    "    # Softmex for probability scaling, i.e. sum of all probabilities = 1    \n",
    "    content_scores = torch.nn.functional.softmax(content_outputs.logits, dim=-1)\n",
    "    # Define the scale and shift it, in this case to --> -1, -0.5, 0, 0.5, 1\n",
    "    weights = torch.linspace(-1, 1, content_scores.shape[1])\n",
    "    normalised_content_scores = torch.matmul(content_scores, weights).tolist()\n",
    "    \n",
    "\n",
    "    # For title content\n",
    "    title_inputs = tokenizer(title_sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():       \n",
    "        title_outputs = model(**title_inputs)\n",
    "    title_scores = torch.nn.functional.softmax(title_outputs.logits, dim=-1)\n",
    "    normalised_title_scores = torch.matmul(title_scores, weights).tolist()\n",
    "\n",
    "    mentions = 0\n",
    "    score = 0.0\n",
    "    count = 0\n",
    "    for sentence, sentiment in zip(content_sentences, normalised_content_scores):\n",
    "        multiplier = 1\n",
    "        if name.lower() in sentence.lower():\n",
    "            mentions += sentence.lower().count(name.lower())\n",
    "            multiplier = 5\n",
    "\n",
    "        count += multiplier\n",
    "        score += sentiment * multiplier\n",
    "\n",
    "    for sentence, sentiment in zip(title_sentences, normalised_title_scores):\n",
    "        mentions += sentence.lower().count(name.lower())\n",
    "        multiplier = 5\n",
    "        count += multiplier\n",
    "        score += sentiment * multiplier\n",
    "\n",
    "    average_score = score / count\n",
    "    article_score = average_score * 50 + 50\n",
    "\n",
    "    return round(article_score, 5), mentions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.4: Combine everything into one pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(html_content:str, title_sentences:str, name:str, language:str, algorithm:str, model, tokenizer):\n",
    "    summary = _extractive_summary(text = clean_string(html_content), focus = name.lower(),  key_phrases= 8, no_of_sentences = 12, lang=language, algo = algorithm)\n",
    "    s_score, _ =_calculate_sentiment(summary, title_sentences, name, model, tokenizer )\n",
    "    return s_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_text = \"What is Volkswagen accused of?\\nIt's been dubbed the \\\"diesel dupe\\\". In September, the Environmental Protection Agency (EPA) found that many VW cars being sold in America had a \\\"defeat device\\\" - or software - in diesel engines that could detect when they were being tested, changing the performance accordingly to improve results. The German car giant has since admitted cheating emissions tests in the US.\\nVW has had a major push to sell diesel cars in the US, backed by a huge marketing campaign trumpeting its cars' low emissions. The EPA's findings cover 482,000 cars in the US only, including the VW-manufactured Audi A3, and the VW models Jetta, Beetle, Golf and Passat. But VW has admitted that about 11 million cars worldwide, including eight million in Europe, are fitted with the so-called \\\"defeat device\\\".\\nThe company has also been accused by the EPA of modifying software on the 3 litre diesel engines fitted to some Porsche and Audi as well as VW models. VW has denied the claims, which affect at least 10,000 vehicles.\\nIn November, VW said it had found \\\"irregularities\\\" in tests to measure carbon dioxide emissions levels that could affect about 800,000 cars in Europe - including petrol vehicles. However, in December it said that following investigations, it had established that this only affected about 36,000 of the cars it produces each year.\\nThis 'defeat device' sounds like a sophisticated piece of kit.\\nFull details of how it worked are sketchy, although the EPA has said that the engines had computer software that could sense test scenarios by monitoring speed, engine operation, air pressure and even the position of the steering wheel.\\nWhen the cars were operating under controlled laboratory conditions - which typically involve putting them on a stationary test rig - the device appears to have put the vehicle into a sort of safety mode in which the engine ran below normal power and performance. Once on the road, the engines switched out of this test mode.\\nThe result? The engines emitted nitrogen oxide pollutants up to 40 times above what is allowed in the US.\\nWhat has been VW's response?\\n\\\"We've totally screwed up,\\\" said VW America boss Michael Horn, while the group's chief executive at the time, Martin Winterkorn, said his company had \\\"broken the trust of our customers and the public\\\". Mr Winterkorn resigned as a direct result of the scandal and was replaced by Matthias Mueller, the former boss of Porsche.\\n\\\"My most urgent task is to win back trust for the Volkswagen Group - by leaving no stone unturned,\\\" Mr Mueller said on taking up his new post.\\nVW has also launched an internal inquiry.\\nBut that's unlikely to be the end of the financial impact. The EPA has the power to fine a company up to $37,500 for each vehicle that breaches standards - a maximum fine of about $18bn.\\nThe costs of possible legal action by car owners and shareholders \\\"cannot be estimated at the current time\\\", VW added.\\nHow widespread are VW's problems?\\nWhat started in the US has spread to a growing number of countries. The UK, Italy, France, South Korea, Canada and, of course, Germany, have opened investigations. Throughout the world, politicians, regulators and environmental groups are questioning the legitimacy of VW's emissions testing.\\nVW will recall 8.5 million cars in Europe, including 2.4 million in Germany and 1.2 million in the UK, and 500,000 in the US as a result of the emissions scandal.\\nNo wonder the carmaker's shares have fallen by about a third since the scandal broke.\\nWill more heads roll?\\nIt's still unclear who knew what and when, although VW must have had a chain of management command that approved fitting cheating devices to its engines, so further departures are likely.\\nChristian Klingler, a management board member and head of sales and marketing is leaving the company, although VW said this was part of long-term planned structural changes and was not related to recent events.\\nIn 2014, in the US, regulators raised concerns about VW emissions levels, but these were dismissed by the company as \\\"technical issues\\\" and \\\"unexpected\\\" real-world conditions. If executives and managers wilfully misled officials (or their own VW superiors) it's difficult to see them surviving.\\nAre other carmakers implicated?\\nThat's for the various regulatory and government inquiries to determine. California's Air Resources Board is now looking into other manufacturers' testing results. Ford, BMW and Renault-Nissan have said they did not use \\\"defeat devices\\\", while other firms have either not commented or simply stated that they comply with the law.\\nThe UK trade body for the car industry, the SMMT, said: \\\"The EU operates a fundamentally different system to the US - with all European tests performed in strict conditions as required by EU law and witnessed by a government-appointed independent approval agency.\\\"\\nBut it added: \\\"The industry acknowledges that the current test method is outdated and is seeking agreement from the European Commission for a new emissions test that embraces new testing technologies and is more representative of on-road conditions.\\\"\\nThat sounds like EU testing rules need tightening, too.\\nEnvironmental campaigners have long argued that emissions rules are being flouted. \\\"Diesel cars in Europe operate with worse technology on average than the US,\\\" said Jos Dings, from the pressure group Transport & Environment. \\\"Our latest report demonstrated that almost 90% of diesel vehicles didn't meet emission limits when they drive on the road. We are talking millions of vehicles.\\\"\\nCar analysts at the financial research firm Bernstein agree that European standards are not as strict as those in the US. However, the analysts said in a report that there was, therefore, \\\"less need to cheat\\\". So, if other European carmakers' results are suspect, Bernstein says the \\\"consequences are likely to be a change in the test cycle rather than legal action and fines\\\".\\nIt's all another blow for the diesel market.\\nCertainly is. Over the past decade and more, carmakers have poured a fortune into the production of diesel vehicles - with the support of many governments - believing that they are better for the environment. Latest scientific evidence suggests that's not the case, and there are even moves to limit diesel cars in some cities.\\nDiesel sales were already slowing, so the VW scandal came at a bad time. \\\"The revelations are likely to lead to a sharp fall in demand for diesel engine cars,\\\" said Richard Gane, automotive expert at consultants Vendigital.\\n\\\"In the US, the diesel car market currently represents around 1% of all new car sales and this is unlikely to increase in the short to medium term.\\n\\\"However, in Europe the impact could be much more significant, leading to a large tranche of the market switching to petrol engine cars virtually overnight.\\\"\"\n",
    "summary = \"In September, the Environmental Protection Agency (EPA) found that many VW cars being sold in America had a \\\"defeat device\\\" - or software - in diesel engines that could detect when they were being tested, changing the performance accordingly to improve results. VW has had a major push to sell diesel cars in the US, backed by a huge marketing campaign trumpeting its cars' low emissions. The company has also been accused by the EPA of modifying software on the 3 litre diesel engines fitted to some Porsche and Audi as well as VW models. In November, VW said it had found \\\"irregularities\\\" in tests to measure carbon dioxide emissions levels that could affect about 800,000 cars in Europe - including petrol vehicles. VW will recall 8.5 million cars in Europe, including 2.4 million in Germany and 1.2 million in the UK, and 500,000 in the US as a result of the emissions scandal.\"\n",
    "title = \"Volkswagen: The scandal explained - BBC News\"\n",
    "name = \"Volkswagen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.7015"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(html_content=html_text, title_sentences=title, name=name, language='en', algorithm=algo_name, model = model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Volkswagen accused of?\n",
      "\n",
      "\"My most urgent task is to win back trust for the Volkswagen Group - by leaving no stone unturned,\" Mr Mueller said on taking up his new post.\n",
      "Mr Winterkorn resigned as a direct result of the scandal and was replaced by Matthias Mueller, the former boss of Porsche.\n",
      "\"The revelations are likely to lead to a sharp fall in demand for diesel engine cars,\" said Richard Gane, automotive expert at consultants Vendigital.\n",
      "In September, the Environmental Protection Agency (EPA) found that many VW cars being sold in America had a \"defeat device\" - or software - in diesel engines that could detect when they were being tested, changing the performance accordingly to improve results.\n",
      "\n",
      "It's been dubbed the \"diesel dupe\".\n",
      "The German car giant has since admitted cheating emissions tests in the US.\n",
      "\n",
      "VW has had a major push to sell diesel cars in the US, backed by a huge marketing campaign trumpeting its cars' low emissions.\n",
      "The EPA's findings cover 482,000 cars in the US only, including the VW-manufactured Audi A3, and the VW models Jetta, Beetle, Golf and Passat.\n",
      "But VW has admitted that about 11 million cars worldwide, including eight million in Europe, are fitted with the so-called \"defeat device\".\n",
      "\n",
      "The company has also been accused by the EPA of modifying software on the 3 litre diesel engines fitted to some Porsche and Audi as well as VW models.\n",
      "VW has denied the claims, which affect at least 10,000 vehicles.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_en(html_text)\n",
    "\n",
    "focus = \"Volkswagen\"\n",
    "doc._.textrank.change_focus(focus,bias=10.0,  default_bias=0.0)\n",
    "for sent in doc._.textrank.summary(limit_phrases=8, limit_sentences=12):\n",
    "    print(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
